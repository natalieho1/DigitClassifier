{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-24T20:09:48.515075Z","iopub.execute_input":"2024-06-24T20:09:48.516520Z","iopub.status.idle":"2024-06-24T20:09:49.037777Z","shell.execute_reply.started":"2024-06-24T20:09:48.516459Z","shell.execute_reply":"2024-06-24T20:09:49.036525Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/email-spam-classification-dataset-csv/emails.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split \nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, confusion_matrix ","metadata":{"execution":{"iopub.status.busy":"2024-06-24T20:09:50.006460Z","iopub.execute_input":"2024-06-24T20:09:50.007511Z","iopub.status.idle":"2024-06-24T20:09:50.863718Z","shell.execute_reply.started":"2024-06-24T20:09:50.007455Z","shell.execute_reply":"2024-06-24T20:09:50.861694Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Create a DataFrame ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/email-spam-classification-dataset-csv/emails.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-24T20:09:54.827888Z","iopub.execute_input":"2024-06-24T20:09:54.828345Z","iopub.status.idle":"2024-06-24T20:09:56.673844Z","shell.execute_reply.started":"2024-06-24T20:09:54.828313Z","shell.execute_reply":"2024-06-24T20:09:56.671762Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"1 = spam, 0 = not spam","metadata":{}},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-06-24T20:09:58.413994Z","iopub.execute_input":"2024-06-24T20:09:58.414384Z","iopub.status.idle":"2024-06-24T20:09:58.457830Z","shell.execute_reply.started":"2024-06-24T20:09:58.414354Z","shell.execute_reply":"2024-06-24T20:09:58.456597Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"       Email No.  the  to  ect  and  for  of    a  you  hou  ...  connevey  \\\n0        Email 1    0   0    1    0    0   0    2    0    0  ...         0   \n1        Email 2    8  13   24    6    6   2  102    1   27  ...         0   \n2        Email 3    0   0    1    0    0   0    8    0    0  ...         0   \n3        Email 4    0   5   22    0    5   1   51    2   10  ...         0   \n4        Email 5    7   6   17    1    5   2   57    0    9  ...         0   \n...          ...  ...  ..  ...  ...  ...  ..  ...  ...  ...  ...       ...   \n5167  Email 5168    2   2    2    3    0   0   32    0    0  ...         0   \n5168  Email 5169   35  27   11    2    6   5  151    4    3  ...         0   \n5169  Email 5170    0   0    1    1    0   0   11    0    0  ...         0   \n5170  Email 5171    2   7    1    0    2   1   28    2    0  ...         0   \n5171  Email 5172   22  24    5    1    6   5  148    8    2  ...         0   \n\n      jay  valued  lay  infrastructure  military  allowing  ff  dry  \\\n0       0       0    0               0         0         0   0    0   \n1       0       0    0               0         0         0   1    0   \n2       0       0    0               0         0         0   0    0   \n3       0       0    0               0         0         0   0    0   \n4       0       0    0               0         0         0   1    0   \n...   ...     ...  ...             ...       ...       ...  ..  ...   \n5167    0       0    0               0         0         0   0    0   \n5168    0       0    0               0         0         0   1    0   \n5169    0       0    0               0         0         0   0    0   \n5170    0       0    0               0         0         0   1    0   \n5171    0       0    0               0         0         0   0    0   \n\n      Prediction  \n0              0  \n1              0  \n2              0  \n3              0  \n4              0  \n...          ...  \n5167           0  \n5168           0  \n5169           1  \n5170           1  \n5171           0  \n\n[5172 rows x 3002 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Email No.</th>\n      <th>the</th>\n      <th>to</th>\n      <th>ect</th>\n      <th>and</th>\n      <th>for</th>\n      <th>of</th>\n      <th>a</th>\n      <th>you</th>\n      <th>hou</th>\n      <th>...</th>\n      <th>connevey</th>\n      <th>jay</th>\n      <th>valued</th>\n      <th>lay</th>\n      <th>infrastructure</th>\n      <th>military</th>\n      <th>allowing</th>\n      <th>ff</th>\n      <th>dry</th>\n      <th>Prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Email 1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Email 2</td>\n      <td>8</td>\n      <td>13</td>\n      <td>24</td>\n      <td>6</td>\n      <td>6</td>\n      <td>2</td>\n      <td>102</td>\n      <td>1</td>\n      <td>27</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Email 3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Email 4</td>\n      <td>0</td>\n      <td>5</td>\n      <td>22</td>\n      <td>0</td>\n      <td>5</td>\n      <td>1</td>\n      <td>51</td>\n      <td>2</td>\n      <td>10</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Email 5</td>\n      <td>7</td>\n      <td>6</td>\n      <td>17</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>57</td>\n      <td>0</td>\n      <td>9</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5167</th>\n      <td>Email 5168</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>32</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5168</th>\n      <td>Email 5169</td>\n      <td>35</td>\n      <td>27</td>\n      <td>11</td>\n      <td>2</td>\n      <td>6</td>\n      <td>5</td>\n      <td>151</td>\n      <td>4</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5169</th>\n      <td>Email 5170</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5170</th>\n      <td>Email 5171</td>\n      <td>2</td>\n      <td>7</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>28</td>\n      <td>2</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5171</th>\n      <td>Email 5172</td>\n      <td>22</td>\n      <td>24</td>\n      <td>5</td>\n      <td>1</td>\n      <td>6</td>\n      <td>5</td>\n      <td>148</td>\n      <td>8</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5172 rows × 3002 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenization ","metadata":{}},{"cell_type":"markdown","source":"Datasets is a library by HuggingFace that allows easily accessing and sharing datasets for Audio, Computer Vision, and NLP tasks. ","metadata":{}},{"cell_type":"markdown","source":"# Pandas Data Frame vs HF Dataset \n* Pandas DataFrame - 2D labeled data structure w columns of potentially different types \n* HF Dataset - library and data structure to store and access datasets ","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n#converts a pandas DataFrame into a Hugging Face Dataset object \nds = Dataset.from_pandas(df)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T20:12:08.236497Z","iopub.execute_input":"2024-06-24T20:12:08.236900Z","iopub.status.idle":"2024-06-24T20:12:13.195692Z","shell.execute_reply.started":"2024-06-24T20:12:08.236870Z","shell.execute_reply":"2024-06-24T20:12:13.194496Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a model developed by Microsoft called DeBERTa. It's the third version and has fewer parameters than the 'base' or 'large versions. ","metadata":{}},{"cell_type":"code","source":"model_nm = 'microsoft/deberta-v3-small'","metadata":{"execution":{"iopub.status.busy":"2024-06-24T20:28:25.752617Z","iopub.execute_input":"2024-06-24T20:28:25.753701Z","iopub.status.idle":"2024-06-24T20:28:25.759193Z","shell.execute_reply.started":"2024-06-24T20:28:25.753663Z","shell.execute_reply":"2024-06-24T20:28:25.757762Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Transformers and AutoTokenizer ","metadata":{}},{"cell_type":"markdown","source":"* transformers - HF library with wide range of pre-trained models for NLP \n* AutoModelForSequenceClassification - loads appropriate model architecture \n* AutoTokenizer - loads appropriate tokenizer (processes text so that model can understand) \n* tokenizer splits the input text into tokens --> numbers --> adds any special tokens ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer \ntokz = AutoTokenizer.from_pretrained(model_nm)","metadata":{"execution":{"iopub.status.busy":"2024-06-24T20:28:59.232054Z","iopub.execute_input":"2024-06-24T20:28:59.232487Z","iopub.status.idle":"2024-06-24T20:29:06.203055Z","shell.execute_reply.started":"2024-06-24T20:28:59.232452Z","shell.execute_reply":"2024-06-24T20:29:06.201839Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8638e262025d48d0ade24c9bc26e6e18"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b4adac19fb482d91d86386c94a7c62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f05ac72ee5c480cb0e6d06f4d4d8bdf"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}